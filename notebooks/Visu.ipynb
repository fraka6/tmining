{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "def load_news(fname=None, all_categories=False):\n",
    "    ''' fname is useless but its there to follow the interface convention'''\n",
    "    # Load some categories from the training set\n",
    "    if all_categories:\n",
    "        categories = None\n",
    "    else:\n",
    "        categories = [\n",
    "            'alt.atheism',\n",
    "            'talk.religion.misc',\n",
    "            'comp.graphics',\n",
    "            'sci.space',\n",
    "            ]\n",
    "\n",
    "    print(\"Loading 20 newsgroups dataset for categories:\")\n",
    "    print(categories if categories else \"all\")\n",
    "\n",
    "    data_train = fetch_20newsgroups(subset='train', categories=categories,\n",
    "                                    shuffle=True, random_state=42)\n",
    "\n",
    "    data_test = fetch_20newsgroups(subset='test', categories=categories,\n",
    "                                   shuffle=True, random_state=42)\n",
    "    print('data loaded')\n",
    "\n",
    "    categories = data_train.target_names    # for case categories == None\n",
    "\n",
    "    print(\"%d categories\" % len(categories))\n",
    "    print()\n",
    "\n",
    "    # split a training set and a test set\n",
    "    return data_train, data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# example of clustering visualisation using scikit-learn\n",
    "\n",
    "METHODS = 'random,pca,lda,isomap,lle,mlle,hlle,ltsa,mds,rtree,spectral,tsne'\n",
    "\n",
    "import numpy as np\n",
    "import traceback\n",
    "from time import time\n",
    "from random import choice, random\n",
    "from sklearn import manifold, decomposition, ensemble, lda, random_projection\n",
    "\n",
    "from mlboost.core.pphisto import SortHistogram, Histogram\n",
    "\n",
    "\n",
    "def plot_embedding(X, Y, title=None, n_sample_by_class=2, min_freq=100,\n",
    "                   sampling=.1, legend_outside_box=True, enable_legend_picking=False, source=None):\n",
    "    '''Scale and visualize the embedding vectors'''\n",
    "\n",
    "    # Make sure there is no line with NaN value\n",
    "    nans = np.isnan(X)\n",
    "    retained = np.invert(np.any(nans, 1))\n",
    "    retained_idx = np.where(retained)[0]\n",
    "    X = X[retained_idx, :]\n",
    "    if len(X) == 0:\n",
    "        # Nothing to plot\n",
    "        return False\n",
    "\n",
    "    import pylab as pl\n",
    "    y = np.array(Y,dtype=str)\n",
    "    x_min, x_max = np.min(X, 0), np.max(X, 0)\n",
    "    X = (X - x_min) / (x_max - x_min)\n",
    "\n",
    "    def get_point_info(x,y):\n",
    "        for i, row in enumerate(X):\n",
    "            if row[0]==x and row[1]==y:\n",
    "                msg = \"[{x},{y}] = row {i}\".format(x=x,y=y,i=i)\n",
    "                if source:\n",
    "                    msg+='-> class : {c}\\n{content}'.format(c=Y[i],content=source[i])\n",
    "                print msg\n",
    "\n",
    "    fig = pl.figure()\n",
    "    ax = pl.subplot(111)\n",
    "\n",
    "    lines = [] # this is used to enable picking on legend (see below)\n",
    "\n",
    "    # show untagged data points\n",
    "    tidx = np.where(y=='?')[0]\n",
    "    if len(tidx):\n",
    "        sX = X[tidx,:]\n",
    "        sX = sX[[i for i in range(len(tidx)) if random()<sampling],:]\n",
    "        if len(sX):\n",
    "            lines.append(ax.plot(sX[:,0],sX[:,1],'.', label='?', picker=True)[0])\n",
    "\n",
    "    # show some tagged samples\n",
    "    tidx = np.where(y!='?')[0]\n",
    "\n",
    "    dist = Histogram(y[tidx])\n",
    "    sdist = SortHistogram(dist, False, True)\n",
    "    classes = [label for label, count in sdist if count>min_freq]\n",
    "    classes.sort()\n",
    "\n",
    "    for cl in classes:\n",
    "        tidx = np.where(y==str(cl))[0]\n",
    "        cX = X[tidx,:]\n",
    "        color = float(classes.index(cl))/len(classes)\n",
    "        lines.append(ax.plot(cX[:,0],cX[:,1],'.', color=pl.cm.Set1(color), label=cl, picker=True)[0])\n",
    "\n",
    "        # show some label on the graph\n",
    "        for i in range(n_sample_by_class):\n",
    "            if len(cX)>0:\n",
    "                idx = choice(range(len(cX)))\n",
    "                pl.text(cX[idx, 0], cX[idx, 1], str(cl),\n",
    "                        color=pl.cm.Set1(float(classes.index(cl))/len(classes)),\n",
    "                        fontdict={'weight': 'bold', 'size': 9})\n",
    "\n",
    "\n",
    "    if legend_outside_box:\n",
    "        # Shink current axis by 20%\n",
    "        box = ax.get_position()\n",
    "        ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])\n",
    "\n",
    "        # Put a legend to the right of the current axis\n",
    "        leg = ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    else:\n",
    "        leg = pl.legend()\n",
    "\n",
    "    if enable_legend_picking:\n",
    "        # enable picking on the legend\n",
    "        lined = dict()\n",
    "\n",
    "        for legline, origline in zip(leg.get_lines(), lines):\n",
    "            legline.set_picker(5)  # 5 pts tolerance\n",
    "            lined[legline] = origline\n",
    "\n",
    "        def onpick(event):\n",
    "            # on the pick event, find the orig line corresponding to the\n",
    "            # legend proxy line, and toggle the visibility\n",
    "            legline = event.artist\n",
    "            origline = lined[legline]\n",
    "            vis = not origline.get_visible()\n",
    "            origline.set_visible(vis)\n",
    "            # Change the alpha on the line in the legend so we can see what lines\n",
    "            # have been toggled\n",
    "            if vis:\n",
    "                legline.set_alpha(1.0)\n",
    "            else:\n",
    "                legline.set_alpha(0.2)\n",
    "            fig.canvas.draw()\n",
    "\n",
    "        fig.canvas.mpl_connect('pick_event', onpick)\n",
    "    else:\n",
    "        # point picking (with data)\n",
    "        from matplotlib.lines import Line2D\n",
    "        def onpick1(event):\n",
    "            if isinstance(event.artist, Line2D):\n",
    "                thisline = event.artist\n",
    "                xdata = thisline.get_xdata()\n",
    "                ydata = thisline.get_ydata()\n",
    "                ind = event.ind\n",
    "                x=np.take(xdata, ind)\n",
    "                y=np.take(ydata, ind)\n",
    "                n=len(x)+1\n",
    "                xy = zip(x,y)\n",
    "                print('%i points: ' %n)\n",
    "                for i,(x,y) in enumerate(xy):\n",
    "                    print(\"#%i)\" %(i+1))\n",
    "                    get_point_info(x, y)\n",
    "\n",
    "        fig.canvas.mpl_connect('pick_event', onpick1)\n",
    "\n",
    "    if title is not None:\n",
    "        pl.title(title)\n",
    "\n",
    "    pl.xlabel('dim 1')\n",
    "    pl.ylabel('dim 2')\n",
    "    return True\n",
    "\n",
    "def randomp(X, dim=2, **kargs):\n",
    "    '''Random 2D projection using a random unitary matrix'''\n",
    "    print(\"Computing random projection\")\n",
    "    try:\n",
    "        rp = random_projection.SparseRandomProjection(n_components=dim, random_state=42)\n",
    "        X_projected = rp.fit_transform(X)\n",
    "        return rp, X_projected, \"Random Projection\"\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "\n",
    "def pca(X, dim=2, **kargs):\n",
    "    '''Projection on to the first 2 principal components'''\n",
    "\n",
    "    print(\"Computing PCA projection\")\n",
    "    try:\n",
    "        pca = decomposition.RandomizedPCA(n_components=dim)\n",
    "        X_pca = pca.fit_transform(X)\n",
    "        return pca, X_pca, \"Principal Components projection\"\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "\n",
    "class ldaModel(object):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def getInvertible(self, X):\n",
    "        X2 = X.copy()\n",
    "        X2.flat[::X.shape[1] + 1] += 0.01  # Make X invertible\n",
    "        return X2\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X2 = self.getInvertible(X)\n",
    "        return self.model.fit(X2, y)\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        X2 = self.getInvertible(X)\n",
    "        return self.model.fit_transform(X2, y)\n",
    "\n",
    "    def transform(self, X):\n",
    "        X2 = self.getInvertible(X)\n",
    "        return self.model.transform(X2)\n",
    "\n",
    "    def predict(self, X):\n",
    "        X2 = self.getInvertible(X)\n",
    "        return self.model.predict(X2)\n",
    "\n",
    "def lda(X, Y, dim=2, **kargs):\n",
    "    '''Projection on to the first 2 linear discriminant components'''\n",
    "    from sklearn import lda\n",
    "    print(\"Computing LDA projection\")\n",
    "    try:\n",
    "        ldatr = ldaModel(lda.LDA(n_components=dim))\n",
    "        X_lda = ldatr.fit_transform(X, Y)\n",
    "        return ldatr, X_lda, \"Linear Discriminant projection of the features\"\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "\n",
    "def isomap(X, dim=2, n_neighbors=30, **kargs):\n",
    "    '''Isomap projection of the dataset'''\n",
    "    print(\"Computing Isomap embedding\")\n",
    "    try:\n",
    "        isomap = manifold.Isomap(n_neighbors, n_components=dim)\n",
    "        X_iso = isomap.fit_transform(X)\n",
    "        print(\"Done.\")\n",
    "        return isomap, X_iso, \"Isomap projection of the features\"\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "\n",
    "def lle(X, dim=2, n_neighbors=30, **kargs):\n",
    "    '''Locally linear embedding of the dataset'''\n",
    "    print(\"Computing LLE embedding\")\n",
    "    methods = ['standard', 'ltsa', 'hessian', 'modified']\n",
    "    clf = manifold.LocallyLinearEmbedding(n_neighbors, n_components=dim,\n",
    "                                          eigen_solver='auto',\n",
    "                                          method='standard')\n",
    "    try:\n",
    "        X_lle = clf.fit_transform(X)\n",
    "        print(\"Done. Reconstruction error: %g\" % clf.reconstruction_error_)\n",
    "        return clf, X_lle, \"Locally Linear Embedding of the features\"\n",
    "\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "\n",
    "def mlle(X, dim=2, n_neighbors=30, **kargs):\n",
    "    '''Modified Locally linear embedding of the dataset'''\n",
    "    print(\"Computing modified LLE embedding\")\n",
    "    clf = manifold.LocallyLinearEmbedding(n_neighbors, n_components=dim,\n",
    "                                          method='modified')\n",
    "    try:\n",
    "        X_mlle = clf.fit_transform(X)\n",
    "        print(\"Done. Reconstruction error: %g\" % clf.reconstruction_error_)\n",
    "        return clf, X_mlle, \"Modified Locally Linear Embedding of the features\"\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "\n",
    "def hlle(X, dim=2, n_neighbors=30, **kargs):\n",
    "    '''HLLE embedding of the dataset'''\n",
    "    print(\"Computing Hessian LLE embedding\")\n",
    "    clf = manifold.LocallyLinearEmbedding(n_neighbors, n_components=dim,\n",
    "                                          method='hessian')\n",
    "    try:\n",
    "        X_hlle = clf.fit_transform(X)\n",
    "        print(\"Done. Reconstruction error: %g\" % clf.reconstruction_error_)\n",
    "        return clf, X_hlle, \"Hessian Locally Linear Embedding of the features\"\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "\n",
    "def ltsa(X, dim=2, n_neighbors=30, **kargs):\n",
    "    '''LTSA embedding of the dataset'''\n",
    "    print(\"Computing LTSA embedding\")\n",
    "    clf = manifold.LocallyLinearEmbedding(n_neighbors, n_components=dim,\n",
    "                                          method='ltsa')\n",
    "    try:\n",
    "        X_ltsa = clf.fit_transform(X)\n",
    "        print(\"Done. Reconstruction error: %g\" % clf.reconstruction_error_)\n",
    "        return clf, X_ltsa, \"Local Tangent Space Alignment of the features\"\n",
    "\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "\n",
    "def mds(X, dim=2, **kargs):\n",
    "    '''MDS  embedding of the dataset'''\n",
    "    print(\"Computing MDS embedding\")\n",
    "    clf = manifold.MDS(n_components=dim, n_init=1, max_iter=100)\n",
    "\n",
    "    try:\n",
    "        X_mds = clf.fit_transform(X)\n",
    "        print(\"Done. Stress: %f\" % clf.stress_)\n",
    "        return clf, X_mds, \"MDS embedding of the features\"\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "\n",
    "def rtree(X, dim=2, n_estimators=200, max_depth=5, **kargs):\n",
    "    '''Random Trees embedding of the dataset'''\n",
    "    print(\"Computing Totally Random Trees embedding\")\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    tr = Pipeline([\n",
    "        ('hasher', ensemble.RandomTreesEmbedding(n_estimators=n_estimators, random_state=0,\n",
    "                                           max_depth=max_depth)),\n",
    "        ('pca', decomposition.RandomizedPCA(n_components=dim))])\n",
    "\n",
    "    try:\n",
    "        X_reduced = tr.fit_transform(X)\n",
    "\n",
    "        return tr, X_reduced, \"Random forest embedding of the features\"\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "def spectral(X, dim=2, **kargs):\n",
    "    # Spectral embedding of the dataset\n",
    "    # quite cool https://github.com/oreillymedia/t-SNE-tutorial\n",
    "    print(\"Computing Spectral embedding\")\n",
    "    embedder = manifold.SpectralEmbedding(n_components=dim, random_state=0,\n",
    "                                          eigen_solver=\"arpack\")\n",
    "    try:\n",
    "        X_se = embedder.fit_transform(X)\n",
    "\n",
    "        return embedder, X_se, \"Spectral embedding of the features\"\n",
    "\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "\n",
    "def tsne(X, dim=2, **kargs):\n",
    "    # t-sne embedding of the dataset\n",
    "    print(\"Computing t-sne embedding\")\n",
    "    embedder = manifold.TSNE(n_components=dim, init='pca', random_state=0)\n",
    "    try:\n",
    "        X_se = embedder.fit_transform(X)\n",
    "\n",
    "        return embedder, X_se, \"t-sne embedding of the features\"\n",
    "\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "\n",
    "_fct_map = {\n",
    "    \"random\":randomp,\n",
    "    \"pca\":pca,\n",
    "    \"lda\":lda,\n",
    "    \"isomap\":isomap,\n",
    "    \"lle\":lle,\n",
    "    \"mlle\":mlle,\n",
    "    \"hlle\":hlle,\n",
    "    \"ltsa\":ltsa,\n",
    "    'tsne':tsne,\n",
    "    # MDS is lacking a transform method, so cannot be used\n",
    "    # to fit a transform on training set and project on the test set.\n",
    "#    \"mds\":mds,\n",
    "    \"rtree\":rtree,\n",
    "    # SpectralEmbedding is lacking a transform method, so cannot be used\n",
    "    # to fit a transform on training set, and project on the test set.\n",
    "#    \"spectral\":spectral\n",
    "    }\n",
    "\n",
    "def dim_reduce(algo_name, **kwargs):\n",
    "    if algo_name not in _fct_map:\n",
    "        print \"warning: %s not available\" %algo_name\n",
    "    else:\n",
    "        return _fct_map[algo_name](**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading /app/.local/share/jupyter/runtime/kernel-fa9a7b8f-7390-4115-8748-69276c766de6.json\n",
      "Loading 20 newsgroups dataset for categories:\n",
      "['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
      "data loaded\n",
      "4 categories\n",
      "()\n",
      "2034 documents - 3.980MB (training set)\n",
      "1353 documents - 2.867MB (test set)\n",
      "Extracting features from the training dataset using a sparse vectorizer\n",
      "done in 1.105438s at 3.600MB/s\n",
      "n_samples: 2034, n_features: 33810\n",
      "()\n",
      "Extracting features from the test dataset using the same vectorizer\n",
      "done in 0.653961s at 4.385MB/s\n",
      "n_samples: 1353, n_features: 33810\n",
      "()\n",
      "data shape: (2034,33810)\n",
      "Computing random projection\n",
      "Projecting random on test set\n",
      "Rendering plot Random Projection (time 3.95s)\n",
      "saving news_random.png\n",
      "Computing PCA projection"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-3-37edecb7a9d7>\", line 153, in pca\n",
      "    X_pca = pca.fit_transform(X)\n",
      "  File \"/app/.heroku/miniconda/lib/python2.7/site-packages/sklearn/decomposition/pca.py\", line 653, in fit_transform\n",
      "    X = self._fit(X)\n",
      "  File \"/app/.heroku/miniconda/lib/python2.7/site-packages/sklearn/decomposition/pca.py\", line 602, in _fit\n",
      "    full_var = np.var(X, axis=0).sum()\n",
      "  File \"/app/.heroku/miniconda/lib/python2.7/site-packages/numpy/core/fromnumeric.py\", line 3089, in var\n",
      "    keepdims=keepdims)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing LDA projection"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/app/.heroku/miniconda/lib/python2.7/site-packages/numpy/core/_methods.py\", line 101, in _var\n",
      "    x = asanyarray(arr - arrmean)\n",
      "MemoryError\n",
      "Traceback (most recent call last):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing Isomap embedding"
     ]
    }
   ],
   "source": [
    "# example of clustering visualisation using scikit-learn\n",
    "\n",
    "import sys\n",
    "import traceback\n",
    "import matplotlib\n",
    "import warnings\n",
    "# load interface; return data_train, data_test that contains data & targets\n",
    "#from news import load_news\n",
    "\n",
    "def filter_classes(X, Y, classes, remove):\n",
    "    ''' remove classes points from X and Y '''\n",
    "    print \"FILTERING CLASSES\"\n",
    "    import numpy as np\n",
    "    cidx=[]\n",
    "    for c in np.array(classes,dtype=int):\n",
    "        cidx.extend(np.where(Y==c)[0])\n",
    "    if remove:\n",
    "        idx=[i for i in range(len(Y)) if i not in cidx]\n",
    "    else:\n",
    "        idx=[i for i in range(len(Y)) if i in cidx]\n",
    "    idx = np.array(idx)\n",
    "    print 'selecting %i/%i indexes' %(len(idx),len(X))\n",
    "    return X[idx], Y[idx]\n",
    "\n",
    "_load_map = {'news':load_news}\n",
    "\n",
    "def add_loading_dataset_fct(name, fct):\n",
    "    if name in _load_map:\n",
    "        print \"load name already used %s\" %name\n",
    "        sys.exit(1)\n",
    "    else:\n",
    "        _load_map[name] = fct\n",
    "\n",
    "def size_mb(docs):\n",
    "    try:\n",
    "        return sum(len(s.encode('utf-8')) for s in docs) / 1e6\n",
    "    #return sum(len(s) for s in docs) / 1e6\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def main(args=None):\n",
    "    args = args or sys.argv[1:]\n",
    "    import logging\n",
    "    import numpy as np\n",
    "    from optparse import OptionParser\n",
    "    from time import time\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer, HashingVectorizer\n",
    "    from sklearn.feature_selection import SelectKBest, chi2\n",
    "    #import dim_reduction as dr\n",
    "\n",
    "    # Display progress logs on stdout\n",
    "    logging.basicConfig(level=logging.INFO,\n",
    "                        format='%(asctime)s %(levelname)s %(message)s')\n",
    "\n",
    "    # parse commandline arguments\n",
    "    op = OptionParser()\n",
    "\n",
    "    op.add_option(\"--chi2_select\", default=-1,\n",
    "                  action=\"store\", type=\"int\", dest=\"select_chi2\",\n",
    "                  help=\"Select some number of features using a chi-squared test; all set -1\")\n",
    "    op.add_option('-f',\"--filename\", default=\"data.tsv\",\n",
    "\t\t  dest=\"fname\", help=\"data filename\")\n",
    "    op.add_option(\"-d\", \"--dataset\", default='news',\n",
    "                  dest=\"dataset\",\n",
    "                  help=\"dataset to load (%s)\" %_load_map.keys())\n",
    "    op.add_option(\"--n_features\",\n",
    "                  action=\"store\", type=int, default=1000,\n",
    "                  help=\"n_features when using the hashing vectorizer.\")\n",
    "    op.add_option(\"--use_hashing\", default=False,\n",
    "                  action=\"store_true\",\n",
    "                  help=\"Use a hashing vectorizer.\")\n",
    "    op.add_option(\"--hack\", default=False,\n",
    "                  action=\"store_true\", dest=\"hack\",\n",
    "                  help=\"use test instead on train to speedup process\")\n",
    "    op.add_option(\"--no-text\", default=True,\n",
    "                  action=\"store_false\", dest=\"text\",\n",
    "                  help=\"features are text\")\n",
    "    op.add_option(\"--class_sample\", default=2, type=int,\n",
    "                  dest=\"n_sample_by_class\",\n",
    "                  help=\"show only [%default%] sample by class\")\n",
    "    op.add_option(\"--lnob\", default=True, action='store_true',\n",
    "                  dest='legend_outside_box',\n",
    "                  help=\"legend not outside of the box\")\n",
    "    op.add_option(\"--legend\", default=False, action='store_true',\n",
    "                  dest='enable_legend_picking',\n",
    "                  help='set legend picking not points')\n",
    "    op.add_option(\"--noX\", default=False, action='store_true',\n",
    "                  dest='nox',\n",
    "                  help=\"if you just want to generate graph and don't have acess to the X server \")\n",
    "    op.add_option(\"-m\",\"--methods\", default=METHODS,\n",
    "                  dest=\"methods\",\n",
    "                  help=\"dimension reduction method to try (split by ','); default = %s\" %METHODS)\n",
    "    op.add_option(\"-e\", dest='exclude', default=None,\n",
    "                  help=\"exclude class (separarated by ,)\")\n",
    "    op.add_option(\"-o\", dest='only', default=None,\n",
    "                  help=\"include only class (separarated by ,)\")\n",
    "    op.add_option(\"-v\", dest='verbose', default=False, action='store_true',\n",
    "                  help=\"verbose\")\n",
    "\n",
    "    (opts, args) = op.parse_args(args)\n",
    "\n",
    "    if len(args) > 0:\n",
    "        op.error(\"this script takes no arguments.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    if opts.nox:\n",
    "        matplotlib.use('Agg')\n",
    "    # warning: pylab should be import after call to matplotlib.use(...)\n",
    "    import pylab\n",
    "\n",
    "    # load data\n",
    "    print \"loading {}\".format(opts.fname)\n",
    "    data_train, data_test = _load_map[opts.dataset](opts.fname)\n",
    "\n",
    "    if opts.hack:\n",
    "        print \"hack: working on test dataset\"\n",
    "        data_train = data_test\n",
    "        opts.dataset+='_test'\n",
    "\n",
    "    if opts.verbose:\n",
    "        print \"----------example data loaded--------------\"\n",
    "        print \"data:\" ,data_train.data[0].strip()\n",
    "        print \"target:\", data_train.target[0]\n",
    "        print \"-------------------------------------------\"\n",
    "    y_train, y_test = data_train.target, data_test.target\n",
    "\n",
    "    data_train_size_mb = size_mb(data_train.data)\n",
    "    data_test_size_mb = size_mb(data_test.data)\n",
    "\n",
    "    print(\"%d documents - %0.3fMB (training set)\" % (\n",
    "            len(data_train.data), data_train_size_mb))\n",
    "    print(\"%d documents - %0.3fMB (test set)\" % (\n",
    "            len(data_test.data), data_test_size_mb))\n",
    "\n",
    "\n",
    "    print(\"Extracting features from the training dataset using a sparse vectorizer\")\n",
    "    t0 = time()\n",
    "    if not opts.text:\n",
    "        X_train = np.array(data_train.data, ndmin=2)\n",
    "        features_names = data_train.features\n",
    "\n",
    "    else: # its test dood\n",
    "        if opts.use_hashing:\n",
    "            print(\"Use feature hashing %s\" %opts.n_features)\n",
    "            vectorizer = HashingVectorizer(stop_words='english', non_negative=True,\n",
    "                                           n_features=opts.n_features)\n",
    "            X_train = vectorizer.transform(data_train.data)\n",
    "        else:\n",
    "            vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,\n",
    "                                         stop_words='english')\n",
    "            # mapping from integer feature name to original token string\n",
    "            X_train = vectorizer.fit_transform(data_train.data)\n",
    "            feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "    if opts.verbose:\n",
    "        print \"----------example data transformed--------------\"\n",
    "        print \"data:\" ,X_train[0]\n",
    "        print \"target:\", y_train[0]\n",
    "        print \"-------------------------------------------\"\n",
    "\n",
    "    duration = time() - t0\n",
    "    print(\"done in %fs at %0.3fMB/s\" % (duration, data_train_size_mb / duration))\n",
    "    print(\"n_samples: %d, n_features: %d\" % X_train.shape)\n",
    "    print()\n",
    "\n",
    "    print(\"Extracting features from the test dataset using the same vectorizer\")\n",
    "    t0 = time()\n",
    "    if not opts.text:\n",
    "        X_test = np.array(data_test.data, ndmin=2)\n",
    "    else:\n",
    "        X_test = vectorizer.transform(data_test.data)\n",
    "    duration = time() - t0\n",
    "    print(\"done in %fs at %0.3fMB/s\" % (duration, data_test_size_mb / duration))\n",
    "    print(\"n_samples: %d, n_features: %d\" % X_test.shape)\n",
    "    print()\n",
    "\n",
    "    if opts.select_chi2!=-1:\n",
    "        print(\"Extracting %d best features by a chi-squared test\" %\n",
    "              opts.select_chi2)\n",
    "        t0 = time()\n",
    "        ch2 = SelectKBest(chi2, k=opts.select_chi2)\n",
    "        print \"data:\", X_train[0]\n",
    "        print \"target\", y_train[0]\n",
    "        X_train = ch2.fit_transform(X_train, y_train)\n",
    "        X_test = ch2.transform(X_test)\n",
    "        print(\"done in %fs\" % (time() - t0))\n",
    "        print()\n",
    "\n",
    "    X = X_train.todense() if \"todense\" in dir(X_train) else X_train\n",
    "    X_test = X_test.todense() if \"todense\" in dir(X_test) else X_test\n",
    "    print \"data shape: (%i,%i)\" %(X.shape)\n",
    "\n",
    "    if opts.only:\n",
    "        idx = opts.only.split(',')\n",
    "        X, y_train = filter_classes(X, y_train, idx, False)\n",
    "        X_test, y_test = filter_classes(X_test, y_test, idx, False)\n",
    "\n",
    "    if opts.exclude:\n",
    "        idx = opts.exclude.split(',')\n",
    "        X, y_train = filter_classes(X, y_train, idx, True)\n",
    "        X_test, y_test = filter_classes(X_test, y_test, idx, True)\n",
    "\n",
    "\n",
    "    # run all dim reduction algo\n",
    "    for method in opts.methods.split(','):\n",
    "        t0 = time()\n",
    "        try:\n",
    "            resdr = dim_reduce(method, X=X, Y=y_train)\n",
    "            if resdr == None:\n",
    "                continue\n",
    "            trans,X_trans,title = resdr\n",
    "            print('Projecting {} on test set'.format(method))\n",
    "            if hasattr(trans,\"transform\"):\n",
    "                X_trans_test = trans.transform(X_test)\n",
    "            elif hasattr(trans,\"fit_transform\"):\n",
    "                warnings.warn(\"the method as no transform (fallback to fit_transform\", Warning)\n",
    "                X_trans_test = trans.fit_transform(X_test)\n",
    "            title = \"%s (time %.2fs)\" %(title,(time() - t0))\n",
    "            print('Rendering plot {}'.format(title))\n",
    "            has_plot = plot_embedding(X=X_trans_test, Y=y_test, title=title,\n",
    "                              n_sample_by_class=opts.n_sample_by_class,\n",
    "                              source=data_test.data,\n",
    "                              legend_outside_box=opts.legend_outside_box,\n",
    "                              enable_legend_picking=opts.enable_legend_picking)\n",
    "            if has_plot:\n",
    "                fname = \"%s_%s.png\" %(opts.dataset, method)\n",
    "                print \"saving %s\" %fname\n",
    "                pylab.savefig(fname, bbox_inches=0)\n",
    "            else:\n",
    "                print('Nothing to plot.')\n",
    "\n",
    "        except Exception, ex:\n",
    "            print method, ex\n",
    "            print traceback.format_exc()\n",
    "\n",
    "    pylab.show()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
